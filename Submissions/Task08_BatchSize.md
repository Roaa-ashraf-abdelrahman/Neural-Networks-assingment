# Task 8 - Batch Size
## conclusion
- as smaller batches introduces lots of variations based on each small amount of data which makes it noisy
- noise is beneficial when we don't wanna be stuck in bad solutions like local minimas, this helps the model explore better solutions
- larger batches converge faster but generalize worse because it may lead to overfitting, performing poorly on new data
- larger batches makes the loss curve smoother
