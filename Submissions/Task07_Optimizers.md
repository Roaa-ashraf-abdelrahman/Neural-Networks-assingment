# Task 7 - Optimizers
## conclusion:
- for some reason, adam optimizers performed worse, maybe number of epochs was too low
- adam outperforam other optimizers because it coverges faster and it adapts both momentum and learning rates without the need to search for the correct values as in SGD
