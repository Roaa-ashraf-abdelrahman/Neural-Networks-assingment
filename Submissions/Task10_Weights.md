# Task 10 - Weights
## conclusion
- large number of parameters because each input connects to every neuron
- more parameters means more flexibility, but that will lead to the model missing the pattern and memorize the training data instead
- dropout randomly drops neurons during training, L2 penalize large weights, early stopping identifies the best weight based on validation loss and stops the training
